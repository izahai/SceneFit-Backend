import faiss
import pickle
import torch
import torch.nn.functional as F
from PIL import Image
from typing import List, Tuple
from pathlib import Path

import app.services.pe_core.vision_encoder.pe as pe
import app.services.pe_core.vision_encoder.transforms as transforms
from app.utils.device import resolve_device, resolve_autocast, resolve_dtype

FAISS_DIR = Path("app/data/faiss")
class PEClipMatcher:
    """
    PE-CLIP matcher supporting:
    - Brute-force matching (original baselines)
    - FAISS-based retrieval (new method)
    """

    TEXT_PROMPT = "a clothing outfit described as: {}"

    def __init__(
        self,
        config_name: str = "PE-Core-B16-224",
        device: str | None = None,
        autocast: bool = True,
        faiss_dir: str | None = FAISS_DIR,
        load_faiss: bool = True,
    ):
        # -------------------------
        # Device / autocast
        # -------------------------
        self.device = resolve_device(device)
        self.device_type = self.device.type
        self.use_autocast = autocast and resolve_autocast(self.device)
        self.autocast_dtype = resolve_dtype(self.device)

        # -------------------------
        # Load model
        # -------------------------
        self.model = pe.CLIP.from_config(
            config_name,
            pretrained=True,
        ).to(self.device)
        self.model.eval()

        # -------------------------
        # Transforms
        # -------------------------
        self.image_transform = transforms.get_image_transform(
            self.model.image_size
        )
        self.text_tokenizer = transforms.get_text_tokenizer(
            self.model.context_length
        )

        # -------------------------
        # Optional FAISS
        # -------------------------
        self.faiss_index = None
        self.faiss_meta = None

        if load_faiss and faiss_dir is not None:
            self._load_faiss(faiss_dir)

    # -------------------------------------------------
    # FAISS loading
    # -------------------------------------------------
    def release(self):
        #self.model.to("cpu")
        del self.model
        torch.cuda.empty_cache()
    def _load_faiss(self, faiss_dir: str):
        faiss_dir = Path(faiss_dir)

        self.faiss_index = faiss.read_index(
            str(faiss_dir / "clothes_image.index")
        )

        with open(faiss_dir / "clothes_image_meta.pkl", "rb") as f:
            self.faiss_meta = pickle.load(f)

    # -------------------------------------------------
    # Text encoding
    # -------------------------------------------------
    @torch.no_grad()
    def encode_text(self, texts: List[str]) -> torch.Tensor:
        if isinstance(texts, str):
            texts = [texts]

        texts = [self.TEXT_PROMPT.format(t) for t in texts]
        tokens = self.text_tokenizer(texts).to(self.device)

        with torch.autocast(
            device_type=self.device_type,
            dtype=self.autocast_dtype,
            enabled=self.use_autocast,
        ):
            _, text_features, _ = self.model(image=None, text=tokens)

        return F.normalize(text_features, dim=-1)

    # -------------------------------------------------
    # Image encoding (still useful!)
    # -------------------------------------------------
    @torch.no_grad()
    def encode_image(self, images: List[Image.Image]) -> torch.Tensor:
        image_tensor = torch.stack(
            [self.image_transform(img) for img in images]
        ).to(self.device)

        with torch.autocast(
            device_type=self.device_type,
            dtype=self.autocast_dtype,
            enabled=self.use_autocast,
        ):
            image_features, _, _ = self.model(image=image_tensor, text=None)

        return F.normalize(image_features, dim=-1)

    # -------------------------------------------------
    # Shared query embedding (NEW, key fix)
    # -------------------------------------------------
    def _build_query_embedding(self, descriptions: List[str]) -> torch.Tensor:
        """
        Aggregate 10 outfit descriptions into ONE query vector.
        """
        text_embs = self.encode_text(descriptions)
        return text_embs.mean(dim=0, keepdim=True)

    # -------------------------------------------------
    # MATCH BY IMAGE (Baseline 1 / FAISS version)
    # -------------------------------------------------
    @torch.no_grad()
    def match_clothes(
        self,
        descriptions: list[str] | None = None,
        query_emb: torch.Tensor | None = None,
        clothes: list[tuple[str, Image.Image]] | None = None,
        top_k: int | None = None,
        lambda_mmr: float = 0.65,      # relevance vs diversity
        candidate_mult: int = 5,       # over-retrieval factor
    ):
        """
        Returns top-k clothes with diversity using MMR.
        Works for both FAISS and brute-force paths.
        """

        if query_emb is None:
            query_emb = self._build_query_embedding(descriptions)

        k = top_k or 10
        candidate_k = k * candidate_mult

        # =====================================================
        # FAISS PATH
        # =====================================================
        if self.faiss_index is not None:
            query_np = query_emb.cpu().numpy().astype("float32")

            scores, indices = self.faiss_index.search(query_np, candidate_k)

            # aggregate max score per clothing item
            best_scores = {}
            for qi in range(scores.shape[0]):
                for score, idx in zip(scores[qi], indices[qi]):
                    score = float(score)
                    if idx not in best_scores or score > best_scores[idx]:
                        best_scores[idx] = score

            cand_indices = list(best_scores.keys())
            cand_scores = torch.tensor(
                [best_scores[i] for i in cand_indices],
                device=query_emb.device
            )

            cand_embs = torch.stack([
                self.faiss_meta["embeddings"][i]
                for i in cand_indices
            ]).to(query_emb.device)

        # =====================================================
        # BRUTE-FORCE PATH
        # =====================================================
        else:
            assert clothes is not None, "Clothes images required for brute-force."

            names, images = zip(*clothes)
            image_embs = self.encode_image(list(images))
            image_embs = F.normalize(image_embs, dim=-1)

            sims = image_embs @ query_emb.T
            cand_scores = sims.max(dim=1).values
            cand_embs = image_embs
            cand_indices = list(range(len(names)))

            # keep only top candidates
            top_vals, top_idx = torch.topk(cand_scores, min(candidate_k, len(cand_scores)))
            cand_scores = top_vals
            cand_embs = cand_embs[top_idx]
            cand_indices = [cand_indices[i] for i in top_idx.tolist()]

        # =====================================================
        # MMR SELECTION
        # =====================================================
        selected = []
        selected_indices = []

        while len(selected_indices) < min(k, len(cand_indices)):
            mmr_scores = []

            for i in range(len(cand_indices)):
                if i in selected_indices:
                    mmr_scores.append(-1e9)
                    continue

                relevance = cand_scores[i]

                if not selected:
                    diversity_penalty = 0.0
                else:
                    sims = cand_embs[i] @ torch.stack(selected).T
                    diversity_penalty = sims.max().item()

                mmr = lambda_mmr * relevance - (1 - lambda_mmr) * diversity_penalty
                mmr_scores.append(mmr)

            best = int(torch.tensor(mmr_scores).argmax())
            selected.append(cand_embs[best])
            selected_indices.append(best)

        # =====================================================
        # FORMAT OUTPUT
        # =====================================================
        results = []
        for i in selected_indices:
            idx = cand_indices[i]
            results.append({
                "name_clothes": (
                    self.faiss_meta["filenames"][idx]
                    if self.faiss_index is not None
                    else names[idx]
                ),
                "similarity": float(cand_scores[i]),
            })

        return results


    # -------------------------------------------------
    # MATCH BY CAPTION (Baseline 2, unchanged semantics)
    # -------------------------------------------------
    @torch.no_grad()
    def match_clothes_captions(
        self,
        descriptions: List[str],
        clothes_captions: dict[str, str],
        top_k: int | None = None,
    ):
        query_emb = self._build_query_embedding(descriptions)

        names = list(clothes_captions.keys())
        captions = list(clothes_captions.values())

        caption_embs = self.encode_text(captions)
        sims = (caption_embs @ query_emb.T).squeeze(1)

        results = [
            {
                "outfit_name": Path(names[i]).stem,
                "similarity": float(sims[i]),
            }
            for i in range(len(names))
        ]

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k] if top_k else results
